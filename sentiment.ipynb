{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFVsClSKN2CQ"
      },
      "source": [
        "# Sentiment 140 Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14kex5l6N2CR"
      },
      "source": [
        "I will be using the sentiment140 dataset from http://help.sentiment140.com/for-students/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82EX4I4N2CS"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSG_F1qON2CS"
      },
      "source": [
        "Get packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz40P4LbN2CS",
        "outputId": "e900ef97-9c7c-4aa5-c87c-e6dbb0352715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v64KNC8TN2CU",
        "outputId": "8fca6098-6aa7-4458-93e4-af95c26db01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkBn2L-jN2CU"
      },
      "source": [
        "Get sentiment140 dataset from http://help.sentiment140.com/for-students/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if using colab\n",
        "!pip install wget\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-deQUrvAOK_q",
        "outputId": "9396a816-59ea-45e5-d4b1-be84cb988325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmsVCzWqN2CU"
      },
      "outputs": [],
      "source": [
        "def download_dataset(url, rename_stem='sentiment140'):\n",
        "    import wget\n",
        "    import zipfile\n",
        "    import os\n",
        "\n",
        "    data_zip_path = f'{rename_stem}.zip'\n",
        "    data_dir = f'{rename_stem}/'\n",
        "\n",
        "    if not os.path.exists(data_zip_path):\n",
        "        wget.download(url, data_zip_path)\n",
        "    if not os.path.exists(data_dir):\n",
        "        with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ0VzjsAN2CU"
      },
      "outputs": [],
      "source": [
        "DATA_URL = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
        "download_dataset(DATA_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7hk6zq2N2CU"
      },
      "source": [
        "## Data Loading and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8z7GkJwN2CV"
      },
      "outputs": [],
      "source": [
        "def cleaner(tweet, valid_words=set()):\n",
        "     import re\n",
        "\n",
        "     # Remove @ sign and @s\n",
        "     tweet = re.sub('@[A-Za-z0-9]+', '', tweet)\n",
        "     tweet = re.sub('@', '', tweet)\n",
        "\n",
        "     # Remove http/https links\n",
        "     tweet = re.sub(r'(?:\\@|http?\\://|https?\\://|www)\\S+', '', tweet)\n",
        "     tweet = ' '.join(tweet.split())\n",
        "     \n",
        "     # Remove hashtag sign but keep the text\n",
        "     tweet = tweet.replace('#', '').replace('_', ' ')\n",
        "\n",
        "     # accept only valid words from nltk\n",
        "     tweet = ' '.join(w for w in nltk.wordpunct_tokenize(tweet) \\\n",
        "                      if w.lower() in valid_words or not w.isalpha())\n",
        "     \n",
        "     tweet = tweet.lower()\n",
        "     \n",
        "     return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxBstsk9N2CV"
      },
      "outputs": [],
      "source": [
        "columns = ['polarity', 'id', 'date', 'query', 'username', 'tweet']\n",
        "df = pd.read_csv('./sentiment140/training.1600000.processed.noemoticon.csv',\n",
        "                 encoding=\"ISO-8859-1\", header=None, names=columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.index[df['polarity'] == 4].tolist())"
      ],
      "metadata": {
        "id": "O1IutoFG42vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_words = set(nltk.corpus.words.words())\n",
        "tqdm.pandas()\n",
        "df['tweet'] = df['tweet'].progress_map(lambda x: cleaner(x, valid_words=valid_words))"
      ],
      "metadata": {
        "id": "S-KbaVXX4Rv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to match output convention later\n",
        "df.loc[df['polarity'] == 4, 'polarity'] = 1\n",
        "print(df['polarity'].unique())"
      ],
      "metadata": {
        "id": "q-pyzh-tU77V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77ZhViQ1N2CV"
      },
      "outputs": [],
      "source": [
        "print('duplicated:', df.duplicated().sum())\n",
        "print('null:', df.notnull().sum(), sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjBfn1YJN2CW"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh-bgX_eN2CW"
      },
      "outputs": [],
      "source": [
        "df.sample(10)[['polarity', 'tweet']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['tweet'].values\n",
        "y = df['polarity'].values"
      ],
      "metadata": {
        "id": "BC5gTGfrYfW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2023)"
      ],
      "metadata": {
        "id": "W3IbvKscY1ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxiK2EVGN2CX"
      },
      "source": [
        "## BERTTokenizer + Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHV4yEmYN2CX"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "print('Loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbqYssH8N2CX"
      },
      "source": [
        "Get longest tokenized sentence (since BERT requires all sentences to be the same length). This includes the special tokens `[CLS]` and `[SEP]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW2n6YULN2CX"
      },
      "outputs": [],
      "source": [
        "# max_len = 0\n",
        "\n",
        "# # For every sentence...\n",
        "# for sample in tqdm(X):\n",
        "\n",
        "#     # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "#     input_ids = tokenizer.encode(sample, add_special_tokens=True)\n",
        "\n",
        "#     # Update the maximum sentence length.\n",
        "#     max_len = max(max_len, len(input_ids))\n",
        "\n",
        "#     del input_ids\n",
        "\n",
        "# print('\\nMax sentence length: ', max_len)\n",
        "\n",
        "# NOTE: the above is on CPU, so uncomment only if using different or new data\n",
        "max_len = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BERT Tokenizer and maxlen to tokenize each sentence by\n",
        "\n",
        "1. tokenize by id\n",
        "2. prepend `[CLS]`\n",
        "3. append `[SEP]` at end\n",
        "4. pad with `[PAD]` tokens until max_len\n",
        "5. attention mask for the `[PAD]` tokens"
      ],
      "metadata": {
        "id": "F--hk-bORyuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "A-36IZc-Woxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertTokenizedTweetsDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 X_train=X_train, y_train=y_train,\n",
        "                 X_test=X_test, y_test=y_test,\n",
        "                 train=True, max_len=256, max_cache_size=800000):\n",
        "        self.df = df\n",
        "        self.train = train\n",
        "        self.max_len = 256\n",
        "\n",
        "        self.X = X_train if self.train else X_test\n",
        "        self.y = y_train if self.train else y_test\n",
        "\n",
        "        self.cache = dict()\n",
        "        self.max_cache_size = max_cache_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if index in self.cache.keys():\n",
        "            return self.cache[index]\n",
        "\n",
        "        X_sample = self.X[index]\n",
        "        y_sample = torch.tensor(self.y[index], dtype=torch.int64)\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                        X_sample,                 # Sentence to encode.\n",
        "                        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "                        max_length=self.max_len,  # Pad & truncate all sentences.\n",
        "                        truncation=True,\n",
        "                        pad_to_max_length=True,\n",
        "                        return_attention_mask=True,   # Construct attn. masks.\n",
        "                        return_tensors='pt',      # Return pytorch tensors.\n",
        "                    )\n",
        "    \n",
        "        X_tokenized_id = encoded_dict['input_ids']\n",
        "        X_mask = encoded_dict['attention_mask']\n",
        "\n",
        "        if len(self.cache) >= self.max_cache_size:\n",
        "            self.cache.popitem()\n",
        "\n",
        "        self.cache[index] = (X_tokenized_id, X_mask, y_sample)\n",
        "\n",
        "        return X_tokenized_id, X_mask, y_sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "metadata": {
        "id": "effpwxNqWW39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BertTokenizedTweetsDataset(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, train=True)\n",
        "test_data = BertTokenizedTweetsDataset(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, train=False)"
      ],
      "metadata": {
        "id": "IYDE2bXyalVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cached_speed(unused_index):\n",
        "    import time\n",
        "    def get_time():\n",
        "        stime = time.time()\n",
        "        x = train_data[unused_index]\n",
        "        etime = time.time()\n",
        "\n",
        "        return etime - stime\n",
        "    \n",
        "    uncached = get_time()\n",
        "    cached = get_time()\n",
        "    speed_x = uncached / cached\n",
        "\n",
        "    print(f'On this trial, cached had {speed_x:.2f}x speed increase')    "
      ],
      "metadata": {
        "id": "TS7UjBSYeU6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cached_speed(np.random.randint(0, len(train_data)-1))"
      ],
      "metadata": {
        "id": "ZYvTcCkMeGg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning w/ BERT"
      ],
      "metadata": {
        "id": "wrlYSMYechs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "train_subset, val_subset = random_split(train_data, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "9QJh4ek7cOQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "CfHtmXa8cgO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Hhd3NVtOcUzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def train_step(bert, dataloader, optimizer, scheduler, epoch=0, val=False, print_batch_every=1000):\n",
        "\n",
        "    # tracking vars\n",
        "    tot_accuracy = 0\n",
        "    tot_loss = 0\n",
        "    batch = 0\n",
        "\n",
        "    # ready for train\n",
        "    if not val:\n",
        "        bert.train()\n",
        "    else:\n",
        "        bert.eval()\n",
        "\n",
        "    prepend = 'val' if val else 'train'\n",
        "\n",
        "    for data in iter(dataloader):\n",
        "\n",
        "        batch += 1\n",
        "\n",
        "        # get data and send to gpu\n",
        "        seqs, masks, labels = data\n",
        "\n",
        "        # align dims\n",
        "        seqs = torch.squeeze(seqs.transpose(2, 1)).to(device)\n",
        "        masks = torch.squeeze(masks.transpose(2, 1)).to(device)\n",
        "        labels = torch.squeeze(labels).to(device)\n",
        "\n",
        "        if not val:\n",
        "            # zero out grads\n",
        "            bert.zero_grad()\n",
        "\n",
        "        # get loss and preds\n",
        "        loss, preds = bert(\n",
        "            seqs,\n",
        "            attention_mask=masks,\n",
        "            labels=labels,\n",
        "            token_type_ids=None,\n",
        "            return_dict=False\n",
        "        )\n",
        "        \n",
        "        # calc and accumulate loss\n",
        "        if not val:\n",
        "            loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "        # torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
        "\n",
        "        if not val:\n",
        "            # descent step + set lr\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        pred_sentiment = preds.data.max(1)[1]\n",
        "        correct = pred_sentiment.eq(labels).cpu().sum()\n",
        "        accuracy = correct.item() / labels.size(0)\n",
        "        tot_accuracy += accuracy\n",
        "\n",
        "        if ((batch - 1) % print_batch_every == 0):\n",
        "            print(f'epoch: {epoch}\\tbatch: {batch}/{len(dataloader)}\\t{prepend}_acc: {accuracy}\\t{prepend}_loss: {loss.item()}')\n",
        "\n",
        "    tot_accuracy = tot_accuracy / len(dataloader)\n",
        "    tot_loss = tot_loss / len(dataloader)\n",
        "\n",
        "    print(f'{prepend} epoch: {epoch}\\t{prepend}_acc: {tot_accuracy}\\t{prepend}_loss: {tot_loss}')\n",
        "    \n",
        "    return tot_accuracy, tot_loss\n",
        "\n",
        "def save(model, optimizer, scheduler, save_path='./bert_sentiment140_fine_tuned.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "    }, save_path)\n",
        "\n",
        "def load(model, optimizer, scheduler, load_path='./bert_sentiment140_fine_tuned.pth'):\n",
        "    checkpoint = torch.load(load_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    return model, optimizer, scheduler\n",
        "\n",
        "def train(bert, train_dl, val_dl, epochs=4, lr=2e-5, eps=1e-8, print_batch_every=1000, save_path='./bert_sentiment140_fine_tuned.pth'):\n",
        "\n",
        "    optimizer = AdamW(bert.parameters(), lr=lr, eps=eps)\n",
        "    \n",
        "    total_steps = len(train_dl) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    train_accs, train_losses, val_accs, val_losses = [], [], [], []\n",
        "\n",
        "    save(bert, optimizer, scheduler, save_path=save_path)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        train_accuracy, train_loss = train_step(\n",
        "            bert, train_dl,\n",
        "            optimizer, scheduler,\n",
        "            epoch=epoch,\n",
        "            print_batch_every=print_batch_every\n",
        "        )\n",
        "\n",
        "\n",
        "        val_accuracy, val_loss = 0, 0\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = train_step(\n",
        "                bert, val_dl,\n",
        "                optimizer, scheduler,\n",
        "                epoch=epoch,\n",
        "                print_batch_every=print_batch_every,\n",
        "                val=True\n",
        "            )\n",
        "\n",
        "        train_accs.append(train_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "        val_accs.append(val_accuracy)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save(bert, optimizer, scheduler, save_path=save_path)\n",
        "\n",
        "    return train_accs, train_losses, val_accs, val_losses"
      ],
      "metadata": {
        "id": "0--1mbER0H00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load BERT model and send to gpu\n",
        "bert = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=len(df['polarity'].unique()),\n",
        "    output_attentions=False, output_hidden_states=False,\n",
        ")\n",
        "bert.to(device)"
      ],
      "metadata": {
        "id": "w6Dk0UtIJ98S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accs, train_losses, val_accs, val_losses = train(bert, train_dl, val_dl, print_batch_every=100)"
      ],
      "metadata": {
        "id": "RK_p4VV8S6H6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bert-transfer-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WxiK2EVGN2CX"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}